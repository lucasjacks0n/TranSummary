{
  "transcript": "2.3:  60 minutes over time.\n5.84:  This week on 60 minutes, we take a look at chat GPT, a new technology fueled by artificial\n13.2:  intelligence that was developed by the company OpenAI.\n18.38:  It is a chatbot that uses massive amounts of data to predict the most likely sequence\n24.52:  of words, allowing it to seem like you're talking to a human.\n29.56:  The bot can answer questions, explain complex topics, and even help write code, emails,\n36.8:  and essays.\n39.52:  Tim Nied Gebrew is the founder and executive director of the Distributed Artificial Intelligence\n45.93:  Research Institute.\n48.16:  She explained to us what she sees as the pitfalls of training artificial intelligence\n54.08:  applications with mountains of indiscriminate data from the internet.\n59.9:  Well, what's start by you just telling us what generative AI is, what does that mean?\n66.16:  So broadly speaking, these are models that take in as input texts mostly, and output\n75.41:  either images or other kinds of texts or even videos.\n80.68:  The generative part means that they're kind of making up texts as they go based on\n85.62:  how they've been trained.\n87.32:  And do all these generative AI systems just swap up everything they can from the internet?\n94.2:  I mean, willy-nilly?\n95.66:  So almost all of them are trained with a lot of either text or images or videos depending\n102.76:  on what they're trained to do from the internet and often indiscriminately.\n108.16:  But many of these companies have found out that that's not acceptable to have as an application.\n114.98:  Systems like ChatGPT have produced outputs that are non-sensical, factually incorrect, even\n123.28:  sexist, racist, or otherwise offensive.\n127.08:  Tim Nied Gebrew was the co-head of Google's AI Ethics team until 2020.\n133.96:  She says she was forced out after she co-authored a paper highlighting the risks of certain\n140.07:  AI systems.\n141.92:  Can you explain to us why we're hearing that output is biased?\n147.76:  There is an assumption by many people who build these types of models that just because\n152.94:  the internet has lots and lots of texts or lots and lots of data, that somehow when you\n158.72:  train something based on that internet, it will encode so many different points of views\n164.64:  and so so much so many diverse views etc.\n166.94:  Right?\n167.42:  And what we argue is that size doesn't guarantee diversity.\n172.88:  There are so many different ways in which people on the internet are bullied off of the\n177.7:  internet.\n178.62:  And now we're going to ask which people are bullied off of the internet.\n181.75:  We know women get harassed online all the time.\n185.08:  We know people in underrepresented groups get harassed and bullied online, right?\n190.5:  When they're bullied, they leave.\n191.89:  They leave.\n193.28:  So they're not represented in what is being stopped up.\n197.82:  Exactly.\n198.22:  So the text that you're using from the internet to train these models is going to be encoding\n205.62:  the people who remain online who are not bullied off all of the sexists and racist\n211.34:  things that are on the internet, all of the hegemonic views that are on the internet.\n216.6:  That's what you're going to be encoding, right?\n218.8:  So we were not surprised to see racist and sexist and homophobic and ableist etc.\n224.99:  Outputs.\n226.44:  Can't you train these systems to identify toxicity and not allow the system to spew it out?\n235.52:  There are many ways in which different organizations and research groups are building\n241.3:  toxicity systems, toxicity detectors.\n244.7:  There are so many different ways in which these models can be toxic, right?\n248.4:  And so many different languages and so many different aspects, so many different tests.\n251.99:  Sometimes it's not even clear.\n253.8:  So actually, similar to social media platforms that do content moderations, they now have\n260.48:  a lot of people that they pay to try and figure out which content is extremely toxic or\n266.54:  horrible so that they can actually train another system that can tell you which is toxic\n272.75:  and not.\n274.26:  Timnit G\u00e8bru says this approach, removing harmful content as it happens is like playing\n281.34:  whack-a-mo.\n282.64:  She thinks the way to handle artificial intelligence systems like these going forward is to\n288.51:  build in oversight and regulation.\n291.82:  Food, medicine, cars, airplanes.\n295.56:  They each have an agency devoted to that.\n298.68:  There's no tech regulation agency.\n303.36:  No.\n303.38:  Should there be?\n304.32:  Yeah.\n304.88:  I absolutely think that there should be an agency that I don't know what it would\n309.87:  look like.\n310.38:  But I do think that there should be an agency that is helping us make sure that some\n317.27:  of these systems are safe, that they're not harming us, that it is actually beneficial.\n323.36:  You know, there should be some sort of oversight.\n325.52:  I don't see any reason why this one industry is being treated so differently from everything\n333.31:  else.",
  "segments": [
    { "timestamp": "2.3", "text": "Introduction to Chat GPT" },
    { "timestamp": "18.38", "text": "Capabilities of Chat GPT" },
    { "timestamp": "39.52", "text": "Interview with Tim Nied Gebrew" },
    { "timestamp": "59.9", "text": "Explanation of Generative AI" },
    { "timestamp": "87.32", "text": "Training AI with Internet Data" },
    { "timestamp": "127.08", "text": "Bias in AI Output" },
    {
      "timestamp": "172.88",
      "text": "Internet Bullying and its Impact on AI Training"
    },
    {
      "timestamp": "226.44",
      "text": "Challenges in Identifying Toxicity in AI Systems"
    },
    { "timestamp": "274.26", "text": "Need for Oversight and Regulation in AI" }
  ],
  "videoId": "kloNp7AAz0U"
}
